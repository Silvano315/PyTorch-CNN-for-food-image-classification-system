import numpy as np
import os
import torch
import logging
import shutil
import sys
import json
from pathlib import Path
import matplotlib.pyplot as plt
from typing import Dict, List, Any, Tuple, Callable, Optional
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import models
from sklearn.metrics import precision_score, recall_score, f1_score


class Experiment:
    """
    A class to manage machine learning experiments, including logging, 
    saving/loading weights, and visualizing training history.
    """
    def __init__(self, name: str, root: str, logger=None):
        self.name = name
        self.root = os.path.join(root, name)
        self.logger = logger
        self.epoch = 1
        self.best_val_loss = sys.float_info.max
        self.best_val_loss_epoch = 1
        self.weights_dir = os.path.join(self.root, 'weights')
        self.history_dir = os.path.join(self.root, 'history')
        self.results_dir = os.path.join(self.root, 'results')
        self.latest_weights = os.path.join(self.weights_dir, 'latest_weights.pth')
        self.latest_optimizer = os.path.join(self.weights_dir, 'latest_optim.pth')
        self.best_weights_path = self.latest_weights
        self.best_optimizer_path = self.latest_optimizer
        self.train_history_fpath = os.path.join(self.history_dir, 'train.csv')
        self.val_history_fpath = os.path.join(self.history_dir, 'val.csv')
        self.test_history_fpath = os.path.join(self.history_dir, 'test.csv')
        self.metrics = ['loss', 'accuracy', 'precision', 'recall', 'f1']
        self.history = {split: {metric: [] for metric in self.metrics} for split in ['train', 'val', 'test']}

    def log(self, msg: str):
        if self.logger:
            self.logger.info(msg)

    def init(self):
        self.log("Creating new experiment")
        self.init_dirs()
        self.init_history_files()

    def resume(self, model: torch.nn.Module, optim: torch.optim.Optimizer, weights_fpath: str = None, optim_path: str = None):
        self.log("Resuming existing experiment")
        if weights_fpath is None:
            weights_fpath = self.latest_weights
        if optim_path is None:
            optim_path = self.latest_optimizer

        model, state = self.load_weights(model, weights_fpath)
        optim = self.load_optimizer(optim, optim_path)

        self.best_val_loss = state['best_val_loss']
        self.best_val_loss_epoch = state['best_val_loss_epoch']
        self.epoch = state['last_epoch'] + 1
        self.load_history_from_file('train')
        self.load_history_from_file('val')

        return model, optim

    def init_dirs(self):
        os.makedirs(self.weights_dir, exist_ok=True)
        os.makedirs(self.history_dir, exist_ok=True)
        os.makedirs(self.results_dir, exist_ok=True)

    def init_history_files(self):
        header = ','.join(['epoch'] + self.metrics) + '\n'
        for split in ['train', 'val', 'test']:
            fpath = getattr(self, f'{split}_history_fpath')
            with open(fpath, 'w') as f:
                f.write(header)

    def load_history_from_file(self, split: str):
        fpath = getattr(self, f'{split}_history_fpath')
        data = np.loadtxt(fpath, delimiter=',', skiprows=1)
        for i, metric in enumerate(self.metrics):
            self.history[split][metric] = data[:, i+1].tolist()

    def save_history(self, split: str, **kwargs):
        for metric, value in kwargs.items():
            metric_name = metric[4:] if metric.startswith('val_') else metric
            if metric_name not in self.history[split]:
                self.history[split][metric_name] = []
            self.history[split][metric_name].append(value)
        fpath = getattr(self, f'{split}_history_fpath')
        with open(fpath, 'a') as f:
            values = [str(kwargs.get(metric, kwargs.get(f'val_{metric}', ''))) for metric in self.metrics]
            f.write(f"{self.epoch},{','.join(values)}\n")
        if split == 'val' and 'loss' in kwargs:
            if self.is_best_loss(kwargs['loss']):
                self.best_val_loss = kwargs['loss']
                self.best_val_loss_epoch = self.epoch

    def is_best_loss(self, loss: float) -> bool:
        return loss < self.best_val_loss

    def save_weights(self, model: torch.nn.Module, **kwargs):
        weights_fname = f"{self.name}-weights-{self.epoch}-" + "-".join([f"{v:.3f}" for v in kwargs.values()]) + ".pth"
        weights_fpath = os.path.join(self.weights_dir, weights_fname)
        try:
            torch.save({
                'last_epoch': self.epoch,
                'best_val_loss': self.best_val_loss,
                'best_val_loss_epoch': self.best_val_loss_epoch,
                'experiment': self.name,
                'state_dict': model.state_dict(),
                **kwargs
            }, weights_fpath)
            shutil.copyfile(weights_fpath, self.latest_weights)
            if self.is_best_loss(kwargs['val_loss']):
                self.best_weights_path = weights_fpath
            self.log(f"Successfully saved weights to {weights_fpath}")
        except Exception as e:
            self.log(f"Error saving weights: {str(e)}")
            raise

    def load_weights(self, model: torch.nn.Module, fpath: str):
        self.log(f"Loading weights from '{fpath}'")
        try:
            state = torch.load(fpath)
            model.load_state_dict(state['state_dict'])
            self.log(f"Loaded weights from experiment {self.name} (last_epoch {state['last_epoch']})")
            return model, state
        except FileNotFoundError:
            self.log(f"Error: Weights file not found at {fpath}")
            raise
        except RuntimeError as e:
            self.log(f"Error loading state dict: {str(e)}")
            raise

    def save_optimizer(self, optimizer: torch.optim.Optimizer, val_loss: float):
        optim_fname = f"{self.name}-optim-{self.epoch}.pth"
        optim_fpath = os.path.join(self.weights_dir, optim_fname)
        try:
            torch.save({
                'last_epoch': self.epoch,
                'experiment': self.name,
                'state_dict': optimizer.state_dict()
            }, optim_fpath)
            shutil.copyfile(optim_fpath, self.latest_optimizer)
            if self.is_best_loss(val_loss):
                self.best_optimizer_path = optim_fpath
            self.log(f"Successfully saved optimizer to {optim_fpath}")
        except Exception as e:
            self.log(f"Error saving optimizer: {str(e)}")
            raise

    def load_optimizer(self, optimizer: torch.optim.Optimizer, fpath: str):
        self.log(f"Loading optimizer from '{fpath}'")
        try:
            optim = torch.load(fpath)
            optimizer.load_state_dict(optim['state_dict'])
            self.log(f"Successfully loaded optimizer from session {optim['experiment']}, last_epoch {optim['last_epoch']}")
            return optimizer
        except FileNotFoundError:
            self.log(f"Error: Optimizer file not found at {fpath}")
            raise
        except Exception as e:
            self.log(f"Error loading optimizer: {str(e)}")
            raise

    def plot_history(self):
        for metric in self.metrics:
            fig, ax = plt.subplots(figsize=(10, 6))
            for split in ['train', 'val']:
                ax.plot(self.history[split][metric], label=split.capitalize())
            ax.set_xlabel('Epoch')
            ax.set_ylabel(metric.capitalize())
            ax.legend()
            ax.set_title(f'{self.name} - {metric.capitalize()}')
            plt.savefig(os.path.join(self.history_dir, f'{metric}.png'))
            plt.close()

        fig, axes = plt.subplots(len(self.metrics), 1, figsize=(12, 6*len(self.metrics)))
        for i, metric in enumerate(self.metrics):
            for split in ['train', 'val']:
                axes[i].plot(self.history[split][metric], label=split.capitalize())
            axes[i].set_xlabel('Epoch')
            axes[i].set_ylabel(metric.capitalize())
            axes[i].legend()
            axes[i].set_title(f'{metric.capitalize()}')
        fig.suptitle(f'{self.name} - Training History')
        plt.tight_layout()
        plt.savefig(os.path.join(self.history_dir, 'combined_history.png'))
        plt.close()

    def update_plots(self):
        self.plot_history()

    def calculate_average_metrics(self, split: str, last_n_epochs: int = 5) -> Dict[str, float]:
        """
        Calculate average metrics for the last n epochs.

        Args:
            split (str): The data split to calculate metrics for ('train', 'val', or 'test').
            last_n_epochs (int): Number of last epochs to consider for averaging.

        Returns:
            Dict[str, float]: A dictionary of averaged metrics.
        """
        avg_metrics = {}
        for metric in self.metrics:
            values = self.history[split][metric][-last_n_epochs:]
            avg_metrics[metric] = sum(values) / len(values)
        return avg_metrics

    def export_results_to_json(self, filepath: str):
        """
        Export experiment results to a JSON file.

        Args:
            filepath (str): Path to save the JSON file.
        """
        results = {
            "name": self.name,
            "best_val_loss": self.best_val_loss,
            "best_val_loss_epoch": self.best_val_loss_epoch,
            "final_metrics": {
                split: self.calculate_average_metrics(split) 
                for split in ['train', 'val', 'test']
            },
            "history": self.history
        }
        try:
            with open(filepath, 'w') as f:
                json.dump(results, f, indent=4)
            self.log(f"Successfully exported results to {filepath}")
        except Exception as e:
            self.log(f"Error exporting results to JSON: {str(e)}")
            raise

    def get_best_epoch(self, metric: str = 'val_loss', mode: str = 'min') -> int:
        """
        Get the epoch with the best performance for a given metric.

        Args:
            metric (str): The metric to consider.
            mode (str): 'min' if lower is better, 'max' if higher is better.

        Returns:
            int: The epoch with the best performance.
        """
        values = self.history['val'][metric]
        if mode == 'min':
            best_value = min(values)
        elif mode == 'max':
            best_value = max(values)
        else:
            raise ValueError("Mode must be 'min' or 'max'")
        return values.index(best_value) + 1  

    def plot_learning_rate(self, lr_history: List[float]):
        """
        Plot the learning rate over epochs.

        Args:
            lr_history (List[float]): List of learning rates for each epoch.
        """
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, len(lr_history) + 1), lr_history)
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.title(f'{self.name} - Learning Rate Schedule')
        plt.savefig(os.path.join(self.history_dir, 'learning_rate.png'))
        plt.close()


class Callback:
    def on_epoch_end(self, epoch: int, logs: Dict[str, float]):
        pass

class EarlyStopping(Callback):
    def __init__(self, monitor: str = 'val_loss', min_delta: float = 0, patience: int = 0, 
                 verbose: bool = False, mode: str = 'auto'):
        self.monitor = monitor
        self.patience = patience
        self.verbose = verbose
        self.min_delta = min_delta
        self.wait = 0
        self.stopped_epoch = 0
        self.best = None
        self.mode = mode
        self.monitor_op = None
        self._init_monitor_op()

    def _init_monitor_op(self):
        if self.mode not in ['auto', 'min', 'max']:
            print(f'EarlyStopping mode {self.mode} is unknown, fallback to auto mode.')
            self.mode = 'auto'
        
        if self.mode == 'min' or (self.mode == 'auto' and 'loss' in self.monitor):
            self.monitor_op = np.less
        else:
            self.monitor_op = np.greater

    def on_epoch_end(self, epoch: int, logs: Dict[str, float]) -> bool:
        current = logs.get(self.monitor)
        if current is None:
            print(f"Early stopping conditioned on metric `{self.monitor}` which is not available. "
                  f"Available metrics are: {','.join(list(logs.keys()))}")
            return False

        if self.best is None:
            self.best = current
            self.wait = 0
        elif self.monitor_op(current - self.min_delta, self.best):
            self.best = current
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                if self.verbose:
                    print(f'Epoch {epoch}: early stopping')
                return True
        return False
    

class ModelCheckpoint(Callback):
    def __init__(self, filepath: str, monitor: str = 'val_loss', verbose: int = 0, 
                 save_best_only: bool = False, mode: str = 'auto', save_weights_only: bool = False):
        self.filepath = filepath
        self.monitor = monitor
        self.verbose = verbose
        self.save_best_only = save_best_only
        self.save_weights_only = save_weights_only
        self.mode = mode
        self.monitor_op = None
        self._init_monitor_op()

    def _init_monitor_op(self):
        if self.mode not in ['auto', 'min', 'max']:
            print(f'ModelCheckpoint mode {self.mode} is unknown, fallback to auto mode.')
            self.mode = 'auto'

        if self.mode == 'min' or (self.mode == 'auto' and 'loss' in self.monitor):
            self.monitor_op = np.less
            self.best = float('inf')
        else:
            self.monitor_op = np.greater
            self.best = -float('inf')

    def on_epoch_end(self, epoch: int, logs: Dict[str, float], model: torch.nn.Module):
        current = logs.get(self.monitor)
        if current is None:
            print(f"Can't save best model, metric `{self.monitor}` is not available. "
                  f"Available metrics are: {','.join(list(logs.keys()))}")
            return

        if self.save_best_only:
            if self.monitor_op(current, self.best):
                if self.verbose > 0:
                    print(f'\nEpoch {epoch:05d}: {self.monitor} improved from {self.best:.5f} to {current:.5f}, '
                          f'saving model to {self.filepath}')
                self.best = current
                if self.save_weights_only:
                    torch.save(model.state_dict(), self.filepath)
                else:
                    torch.save(model, self.filepath)
        else:
            if self.verbose > 0:
                print(f'\nEpoch {epoch:05d}: saving model to {self.filepath}')
            if self.save_weights_only:
                torch.save(model.state_dict(), self.filepath)
            else:
                torch.save(model, self.filepath)

class ReduceLROnPlateau(Callback):
    def __init__(self, optimizer: torch.optim.Optimizer, mode: str = 'min', factor: float = 0.1, 
                 patience: int = 10, verbose: bool = False, min_lr: float = 0, eps: float = 1e-8,
                 monitor: str = 'val_loss'):
        self.optimizer = optimizer
        self.mode = mode
        self.factor = factor
        self.patience = patience
        self.verbose = verbose
        self.min_lr = min_lr
        self.eps = eps
        self.monitor = monitor
        self.cooldown_counter = 0
        self.wait = 0
        self.best = None
        self.mode_worse = None
        self.is_better = None
        self._init_is_better(mode)

    def _init_is_better(self, mode):
        if mode not in {'min', 'max'}:
            raise ValueError('mode ' + mode + ' is unknown!')

        if mode == 'min':
            self.mode_worse = float('inf')
            self.is_better = lambda a, best: a < best - self.eps
        if mode == 'max':
            self.mode_worse = -float('inf')
            self.is_better = lambda a, best: a > best + self.eps

    def on_epoch_end(self, epoch: int, logs: Dict[str, float]):
        current = logs.get(self.monitor)
        if current is None:
            print(f"ReduceLROnPlateau conditioned on metric `{self.monitor}` which is not available. "
                  f"Available metrics are: {','.join(list(logs.keys()))}")
            return

        if self.best is None or self.is_better(current, self.best):
            self.best = current
            self.wait = 0
        else:
            self.wait += 1

        if self.wait >= self.patience:
            self._reduce_lr(epoch)
            self.wait = 0

    def _reduce_lr(self, epoch):
        for i, param_group in enumerate(self.optimizer.param_groups):
            old_lr = float(param_group['lr'])
            new_lr = max(old_lr * self.factor, self.min_lr)
            if old_lr - new_lr > self.eps:
                param_group['lr'] = new_lr
                if self.verbose:
                    print(f'Epoch {epoch}: reducing learning rate of group {i} to {new_lr:.4e}.')


class BaselineCNN(nn.Module):
    def __init__(self, num_classes, input_channels=3):
        super(BaselineCNN, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool = nn.MaxPool2d(2, 2)
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(128, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)
        self.dropout_1 = nn.Dropout(0.1)
        self.dropout_2 = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = self.adaptive_pool(x)
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.dropout_1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout_2(x)
        x = self.fc3(x)
        return x  
    

class EfficientNetTransfer(nn.Module):
    def __init__(self, num_classes: int, efficientnet_version: str = 'b0', pretrained: bool = True):
        super(EfficientNetTransfer, self).__init__()
        
        if efficientnet_version == 'b0':
            efficientnet = models.efficientnet_b0(pretrained=pretrained)
        elif efficientnet_version == 'b1':
            efficientnet = models.efficientnet_b1(pretrained=pretrained)
        
        self.features = nn.Sequential(*list(efficientnet.children())[:-2])
        self.pooling = nn.AdaptiveAvgPool2d((1, 1))        
        num_ftrs = efficientnet.classifier[1].in_features
        self.fc = nn.Sequential(
            nn.Dropout(p=0.2, inplace=True),
            nn.Linear(num_ftrs, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)  
        x = self.pooling(x)  
        x = torch.flatten(x, 1) 
        x = self.fc(x)    
        return x
    
    
def freeze_layers(model: nn.Module, num_layers: int = -1):
    """
    Freeze layers of the model for transfer learning.

    Args:
        model (nn.Module): The model to freeze layers in.
        num_layers (int): Number of layers to freeze from the start. -1 means freeze all except the last layer.
    """
    if isinstance(model, EfficientNetTransfer):
        if num_layers == -1:
            for param in model.features.parameters():
                param.requires_grad = False
        else:
            for i, (name, param) in enumerate(model.features.named_parameters()):
                if i < num_layers:
                    param.requires_grad = False
                else:
                    param.requires_grad = True
    else:
        raise NotImplementedError("Freezing layers is only implemented for EfficientNetTransfer")

def create_model(num_classes: int, model_type: str = 'baseline', **kwargs) -> nn.Module:
    """
    Create a model for transfer learning.

    Args:
        num_classes (int): Number of classes in the dataset.
        model_type (str): Type of model to create ('efficientnet' or 'baseline').
        **kwargs: Additional arguments for the model.

    Returns:
        nn.Module: The created model.
    """
    if model_type == 'efficientnet':
        return EfficientNetTransfer(num_classes, **kwargs)
    elif model_type == 'baseline':
        return BaselineCNN(num_classes, **kwargs)
    else:
        raise ValueError(f"Unknown model type: {model_type}")


def train_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, 
                optimizer: optim.Optimizer, device: torch.device) -> Dict[str, float]:
    """
    Train the model for one epoch.

    Args:
        model (nn.Module): The neural network model to train.
        dataloader (DataLoader): The DataLoader for the training data.
        criterion (nn.Module): The loss function.
        optimizer (optim.Optimizer): The optimizer for updating model parameters.
        device (torch.device): The device to run the training on (CPU or GPU).

    Returns:
        Dict[str, float]: A dictionary containing the average loss and various metrics for the epoch.
    """
    model.train()
    running_loss = 0.0
    predictions: List[np.ndarray] = []
    targets: List[np.ndarray] = []

    for i, (inputs, labels) in enumerate(dataloader):
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())
        targets.extend(labels.cpu().numpy())

    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_accuracy = np.mean(np.array(predictions) == np.array(targets))
    epoch_precision = precision_score(targets, predictions, average='weighted', zero_division=1)
    epoch_recall = recall_score(targets, predictions, average='weighted')
    epoch_f1 = f1_score(targets, predictions, average='weighted')

    return {
        'loss': epoch_loss,
        'accuracy': epoch_accuracy,
        'precision': epoch_precision,
        'recall': epoch_recall,
        'f1': epoch_f1
    }

def validate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, 
             device: torch.device) -> Dict[str, float]:
    """
    Validate the model on the validation set.

    Args:
        model (nn.Module): The neural network model to validate.
        dataloader (DataLoader): The DataLoader for the validation data.
        criterion (nn.Module): The loss function.
        device (torch.device): The device to run the validation on (CPU or GPU).

    Returns:
        Dict[str, float]: A dictionary containing the average loss and various metrics for the validation set.
    """
    model.eval()
    running_loss = 0.0
    predictions: List[np.ndarray] = []
    targets: List[np.ndarray] = []

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloader):
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * inputs.size(0)
            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())
            targets.extend(labels.cpu().numpy())

    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_accuracy = np.mean(np.array(predictions) == np.array(targets))
    epoch_precision = precision_score(targets, predictions, average='weighted', zero_division=1)
    epoch_recall = recall_score(targets, predictions, average='weighted')
    epoch_f1 = f1_score(targets, predictions, average='weighted')

    return {
        'loss': epoch_loss,
        'accuracy': epoch_accuracy,
        'precision': epoch_precision,
        'recall': epoch_recall,
        'f1': epoch_f1
    }


def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,
                experiment: Any, callbacks: List[Any], num_epochs: int,
                device: torch.device, logger: logging.Logger) -> nn.Module:
    """
    Train the model for a specified number of epochs.

    Args:
        model (nn.Module): The neural network model to train.
        train_loader (DataLoader): The DataLoader for the training data.
        val_loader (DataLoader): The DataLoader for the validation data.
        experiment (Any): An object to track the experiment (e.g., for logging).
        callbacks (List[Any]): A list of callback objects for various training events.
        num_epochs (int): The number of epochs to train for.
        device (torch.device): The device to run the training on (CPU or GPU).
        logger (logging.Logger): Logger object for detailed logging.

    Returns:
        nn.Module: The trained model.
    """
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    logger.info(f"Starting training for {num_epochs} epochs")
    logger.info(f"Model: {model.__class__.__name__}")
    logger.info(f"Optimizer: {optimizer.__class__.__name__}")
    logger.info(f"Criterion: {criterion.__class__.__name__}")
    logger.info(f"Device: {device}")

    for epoch in range(1, num_epochs + 1):
        logger.info(f"Epoch {epoch}/{num_epochs}")

        train_logs = train_epoch(model, train_loader, criterion, optimizer, device)
        val_logs = validate(model, val_loader, criterion, device)

        val_logs_prefixed = {'val_' + k: v for k, v in val_logs.items()}
        logs = {**train_logs, **val_logs_prefixed}
        
        log_message = f"Epoch {epoch} - "
        log_message += " | ".join([f"{k}: {v:.4f}" for k, v in logs.items()])
        logger.info(log_message)

        experiment.save_history('train', **train_logs)
        experiment.save_history('val', **val_logs_prefixed)
        experiment.update_plots()

        stop_training = False
        for callback in callbacks:
            if isinstance(callback, ModelCheckpoint):
                callback.on_epoch_end(epoch, logs, model)
                logger.info(f"ModelCheckpoint: Saved model at epoch {epoch}")
            elif isinstance(callback, ReduceLROnPlateau):
                old_lr = optimizer.param_groups[0]['lr']
                callback.on_epoch_end(epoch, logs)
                new_lr = optimizer.param_groups[0]['lr']
                if old_lr != new_lr:
                    logger.info(f"ReduceLROnPlateau: Learning rate changed from {old_lr} to {new_lr}")
            else:
                stop_training = callback.on_epoch_end(epoch, logs)
                if stop_training:
                    logger.info(f"Early stopping triggered at epoch {epoch}")
                    break

        if stop_training:
            break

    logger.info("Training completed")
    return model


def get_predictions(model: torch.nn.Module, dataloader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray]:
    """
    Get predictions from the model for the entire dataset.

    Args:
        model (torch.nn.Module): The trained model to use for predictions.
        dataloader (DataLoader): DataLoader containing the dataset to predict on.
        device (torch.device): The device to run the model on (CPU or GPU).

    Returns:
        Tuple[np.ndarray, np.ndarray]: A tuple containing two numpy arrays:
            - The first array contains the true labels.
            - The second array contains the predicted labels.
    """
    model.eval()
    all_preds: List[int] = []
    all_labels: List[int] = []
    
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(labels.cpu().tolist())
    
    return np.array(all_labels), np.array(all_preds)